[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "PsyLens\n\n\n\nGenAI\n\n\n\nAgent for Psychologists\n\n\n\n\n\nNov 18, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/psylens.html",
    "href": "posts/psylens.html",
    "title": "PsyLens",
    "section": "",
    "text": "You can find the code in this repo."
  },
  {
    "objectID": "posts/psylens.html#resources",
    "href": "posts/psylens.html#resources",
    "title": "PsyLens",
    "section": "",
    "text": "You can find the code in this repo."
  },
  {
    "objectID": "posts/psylens.html#background",
    "href": "posts/psylens.html#background",
    "title": "PsyLens",
    "section": "Background",
    "text": "Background\nThis project is part of my work at TerrablueXT and the objective is to enable Psychologists query/interact with their client’s CRF (Case-Report-Form) response & Mental-health report (PDF).\nAll the details about various approaches & steps to execute the project are explain in the following sections."
  },
  {
    "objectID": "posts/psylens.html#agent",
    "href": "posts/psylens.html#agent",
    "title": "PsyLens",
    "section": "Agent",
    "text": "Agent\n The above image demonstrates the Agent workflow that was implemented for this usecase. It is a mixture of both Routing worflow & Agent (LangChain’s blog) The purpose of each node is as follows:\n\nInput guardrail: This node provides protection through OpenAI’s guardrails at following levels:\n\nModeration: Detect harmful or policy-violating content including hate speech, harassment, self-harm, and other inappropriate content.\nJailbreak: Detect jailbreak attempts by identifying various attack patterns.\nOff-topic prompts: Detect prompts deviating for the core task of the agent.\n\nunrelated_query_handler: Any user-query that trips the Guardrails it routed to this node, which returns a regret message.\nintent_classifier: This node detects the intent of the input query and routes it accordingly to either the greetings or planner nodes.\ngreetings: This node handles the greeting related users messages and returns an appropriate response.\nplanner: This is the main agent responsibel to generate/answer the user’s actual query from the database or PDF reports. It has access to 2 tools: Database-tool & RAG-tool, using which it enriches the LLM context with relevant information to answer user’s query.\nconversation_summary: This node generates a concise summary of the running multi-turn conversation between the user & agent. It is triggered when accumulated number of messages exceeds 6 (3-turns).\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the user here is the Psychologist\n\n\n\nDatabase Tool\n To answer user’s query related to data stored in database i.e. Client’s demographic, health-history, responses to GAD & PHQ questionnaires. To achieve this, I took the following approach:\n\nSQL query generation: Firstly, user’s natural language query is converted to an SQL query, using an LLM API call enriched with the database schema.\nQuery execution: The generated SQL query is then executed using LangChain’s SQLDatabase engine and the fetched results are returned to the Planner agent.\n\n\n\n\n\n\n\nNote\n\n\n\nAs an security measure, I created a user profile with read-only privileges of the database, to run/execute the generated SQL query. This is to prevent executing a malicious query, modifying the database in unwanted ways.\n\n\n\n\nRAG Tool\nRetrieval Augmented Generation is used to augement & enrich the context of LLM to answer queries from domain-specific or personal knowledge base which the LLM hasn’t been trained on. This is a technique to ground the LLM with unseen & specific data and generate responses based on that.\nHere, in this case the knowledge base consists of Psychologist PDF reports, each of which contain the mental-health analysis for a particular client. I am leveraging RAG to let Psychologist interact with these report to reduce the analysis effort during/before 1-on-1 meeting with the client.\nThe PDF reports are quite structured in their format:\n\nMultimodal i.e. Text + Plots/Graphs(images) + Tabular data\nAll the topic headings in each report are the same except the content\n\nSince, the reports are multimodal (as they contain text & images), there are following approaches mentioned in this blog:\n\nEmbed all modalities into the same vector space\nGround all modalities into one primary modality\nHave separate stores for different modalities\n\nI chose the 2nd approach, where I ground all the modalities into the text modality.\n\nPipeline\n The RAG pipeline I used has the following steps:\n\nDocument Parsing\nThis steps involves loading & the parsing the content in the PDF. For this step, Docling package was used to\n\nExtract text content as markdown\nExtract table as markdown (preserves the tabular structure to some extent in text format) because ~90% of PDF is just tables.\nExtract plots-images as base64 encoded strings.\n\nI did try other PDF loaders from LangChain such as PyMuPDF, PDFPlumber & PyMuPDF4LLM, but each of them had issues of their own like unable to extract tables as markdown text, adding extra columns in tables etc.\nDocling provide good results in the above tasks and that’s the reason for this choice.\n\n\nDocument Chunking\nThis is the most crucial step in a RAG pipeline. As the name suggests, we divide the entire document into chunks which are then embedded into a vector store. There are many different chunking strategies to choose from, but keeping document-structure & context preservation between chunks in mind, I chose Document-structure based chunking.\nWhat this steps involves is to chunk the entire document based on the Headers (specifically Heading 2) using MarkdownTextHeaderSplitter from LangChain. The effect: Preserves context of each title in document by keeping all the content of that title together in a single chunk.\nExtra preprocessing:\n\nRemoving Page header from the markdown text extracted during parsing step\nAdding metadata such as file-name, chunk-type(image or text)\n\n\n\nContextual Summary\nThis is an additional step in RAG pipeline suggested in Anthropic’s blog, which results in better retrieval metrics. It involves generating a summary of a particular chunk w.r.t to the entire document. This summary helps locate the chunk in the document and provide necessary context about what the chunk content is about w.r.t the document. The summary is then prepended to original chunk and then embedded into the vector store\nSince, the plot-images need to converted to text itself, I didn’t generate a contextual summary for them.\n\n\nIndexing\nNow, to index the chunks, I experimented along these aspects:\n\nChoice of Embedding model: MTEB leaderboard is a good reference to compare Embedding models’ performance for different tasks. In our case, since retrieval is the main task, I tried out the following Embedding models:\n\nqwen3-embedding-8B: Its open-source and is the top-performer on the leaderboard currently. Accessed this model via Nebius’s API, due to resource constraints.\nqwen3-embedding-4B: Its open-source and provides great balance between performance & memory requirement.\ngemini-embedding-001: Accessible via API and was the top-model before Qwen3 dethroned it.\nvoyage-3-large: Suggested in Anthropic’s blog. This model can be accessed via API and they offer first 200M embedding tokens for free\nmultilingual-e5-large-instruct: Due to smaller size and it has decent performance on retrieval task\n\nChoice of Vector-store: As the name suggests, they are used to store the embedding vectors of all the chunks. I chose Qdrant due to the following reasons:\n\nOpen-source & can be run locally\nWritten in Rust which makes it fast & reliable\nFastest metadata filtering\nHigh throughput\nTo create the retriever, I used LangChain’s MultiVectorRetriever, with QdrantVectorStore as vector-store to index the chunk-embedding-vectors & InMemoryStore as doc-store to store the original document chunks\n\nChoice of Search-type: I experimented with the following search-types:\n\nSemantic search: It involves retrieving documents based on similarity scores between the query-embedding & vector-embeddings in the vector-store\nHybrid search: It combines Semantic search with Keyword-based (BM25) search, retrieving documents using RRF(Reciprocal Rank Fusion) as combination & ranking strategy.\n\n\n\n\nEvaluation\nTo log the performance of the retrieval, I created a evaluation set consisting of queries & golden-chunk-ids(ground-truth) key-value pairs against which the pipeline will be evaluated. The metric I chose in this case was Recall@k, where k={3, 5, 7}.\nFollowing are the results for the experiments conducted:\n\nEmbedding-model & Search-type choice Performance\n\n\n\n\n\n\n\nEmbedding model\nSemantic Search\nHybrid Search\n\n\n\n\nmultilingual-e5-large-instruct\nRecall@3: 81.67%  Recall@5: 90%  Recall@7: 96%\nRecall@3: 82.33%  Recall@5: 95%  Recall@7: 97%\n\n\ngemini-embedding-001\nRecall@3: 77.33%  Recall@5: 88.67%  Recall@7: 95.33%\nRecall@3: 84%  Recall@5: 93%  Recall@7: 97.67%\n\n\nqwen3-embedding-4B\nRecall@3: 79%  Recall@5: 88%  Recall@7: 94.67%\nRecall@3: 83.33%  Recall@5: 93%  Recall@7: 96.33%\n\n\nqwen3-embedding-8B\nRecall@3: 83.67%  Recall@5: 93.67%  Recall@7: 96.33%\nRecall@3: 84.67%  Recall@5: 93.33%  Recall@7: 97.33%\n\n\nvoyage-3-large\nRecall@3: 85%  Recall@5: 95.33%  Recall@7: 98.33%\nRecall@3: 86.67%  Recall@5: 93.67%  Recall@7: 97.33%\n\n\n\n\nReranking Performance\n\n\n\n\n\n\n\nEmbedding model\nWithout Re-ranking\nWith Re-ranking\n\n\n\n\nvoyage-3-large\nRecall@3: 85%  Recall@5: 95.33%  Recall@7: 98.33%\nRecall@3: 79.33%  Recall@5: 88.67%  Recall@7: 95.67%\n\n\n\n\n\nRetrieval\nBased on the results of above experiments, I made the following choices for the RAG retrieval pipeline:\n\nRetrieving only top-5 relevant chunks: This is the sweet-spot between top-3 & top-7 with a balance of observed performance & quantity of relevant information injected into the LLM context.\nVoyage-3-large as the embedding model: From the evaluation performances, Voyage-3-large provided the best metric for top-5 retrieved chunks.\nSemantic search in vector-database: Again from the evaluation performances, semantic search with Voyage-3-large embedding model was better than Hybrid search.\nNo reranking after retrieval: Re-ranking post-retrieval saw a dip in Recall@5 metric performance."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rahul Menon",
    "section": "",
    "text": "Machine Learning & Deep Learning Enthusiast | Love to Code"
  },
  {
    "objectID": "pages/about.html",
    "href": "pages/about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]