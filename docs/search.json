[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Rahul Menon",
    "section": "",
    "text": "LoanTap\n\n\n\nMachine learning\n\nClassification\n\n\n\nCredit Risk Prediction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPsyLens\n\n\n\nGenAI\n\nRAG\n\n\n\nAgent for Psychologists\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/psylens.html",
    "href": "posts/psylens.html",
    "title": "PsyLens",
    "section": "",
    "text": "You can find the code for this project in this repo."
  },
  {
    "objectID": "posts/psylens.html#resources",
    "href": "posts/psylens.html#resources",
    "title": "PsyLens",
    "section": "",
    "text": "You can find the code for this project in this repo."
  },
  {
    "objectID": "posts/psylens.html#background",
    "href": "posts/psylens.html#background",
    "title": "PsyLens",
    "section": "Background",
    "text": "Background\nThis project is part of my work at TerrablueXT and the objective is to enable Psychologists query/interact with their client’s CRF (Case-Report-Form) response & Mental-health report (PDF).\nAll the details about various approaches & steps to execute the project are explain in the following sections."
  },
  {
    "objectID": "posts/psylens.html#agent",
    "href": "posts/psylens.html#agent",
    "title": "PsyLens",
    "section": "Agent",
    "text": "Agent\n\nThe above image demonstrates the Agent workflow that was implemented for this usecase. It is a mixture of both Routing worflow & Agent (LangChain’s blog) The purpose of each node is as follows:\n\nInput guardrail: This node provides protection through OpenAI’s guardrails at following levels:\n\nModeration: Detect harmful or policy-violating content including hate speech, harassment, self-harm, and other inappropriate content.\nJailbreak: Detect jailbreak attempts by identifying various attack patterns.\nOff-topic prompts: Detect prompts deviating for the core task of the agent.\n\nunrelated_query_handler: Any user-query that trips the Guardrails it routed to this node, which returns a regret message.\nintent_classifier: This node detects the intent of the input query and routes it accordingly to either the greetings or planner nodes.\ngreetings: This node handles the greeting related users messages and returns an appropriate response.\nplanner: This is the main agent responsibel to generate/answer the user’s actual query from the database or PDF reports. It has access to 2 tools: Database-tool & RAG-tool, using which it enriches the LLM context with relevant information to answer user’s query.\nconversation_summary: This node generates a concise summary of the running multi-turn conversation between the user & agent. It is triggered when accumulated number of messages exceeds 6 (3-turns).\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the user here is the Psychologist\n\n\n\nDatabase Tool\n To answer user’s query related to data stored in database i.e. Client’s demographic, health-history, responses to GAD & PHQ questionnaires. To achieve this, I took the following approach:\n\nSQL query generation: Firstly, user’s natural language query is converted to an SQL query, using an LLM API call enriched with the database schema.\nQuery execution: The generated SQL query is then executed using LangChain’s SQLDatabase engine and the fetched results are returned to the Planner agent.\n\n\n\n\n\n\n\nNote\n\n\n\nAs an security measure, I created a user profile with read-only privileges of the database, to run/execute the generated SQL query. This is to prevent executing a malicious query, modifying the database in unwanted ways.\n\n\n\n\nRAG Tool\nRetrieval Augmented Generation is used to augement & enrich the context of LLM to answer queries from domain-specific or personal knowledge base which the LLM hasn’t been trained on. This is a technique to ground the LLM with unseen & specific data and generate responses based on that.\nHere, in this case the knowledge base consists of Psychologist PDF reports, each of which contain the mental-health analysis for a particular client. I am leveraging RAG to let Psychologist interact with these report to reduce the analysis effort during/before 1-on-1 meeting with the client.\nThe PDF reports are quite structured in their format:\n\nMultimodal i.e. Text + Plots/Graphs(images) + Tabular data\nAll the topic headings in each report are the same except the content\n\nSince, the reports are multimodal (as they contain text & images), there are following approaches mentioned in this blog:\n\nEmbed all modalities into the same vector space\nGround all modalities into one primary modality\nHave separate stores for different modalities\n\nI chose the 2nd approach, where I ground all the modalities into the text modality.\n\nPipeline\n The RAG pipeline I used has the following steps:\n\nDocument Parsing\nThis steps involves loading & the parsing the content in the PDF. For this step, Docling package was used to\n\nExtract text content as markdown\nExtract table as markdown (preserves the tabular structure to some extent in text format) because ~90% of PDF is just tables.\nExtract plots-images as base64 encoded strings.\n\nI did try other PDF loaders from LangChain such as PyMuPDF, PDFPlumber & PyMuPDF4LLM, but each of them had issues of their own like unable to extract tables as markdown text, adding extra columns in tables etc.\nDocling provide good results in the above tasks and that’s the reason for this choice.\n\n\nDocument Chunking\nThis is the most crucial step in a RAG pipeline. As the name suggests, we divide the entire document into chunks which are then embedded into a vector store. There are many different chunking strategies to choose from, but keeping document-structure & context preservation between chunks in mind, I chose Document-structure based chunking.\nWhat this steps involves is to chunk the entire document based on the Headers (specifically Heading 2) using MarkdownTextHeaderSplitter from LangChain. The effect: Preserves context of each title in document by keeping all the content of that title together in a single chunk.\nExtra preprocessing:\n\nRemoving Page header from the markdown text extracted during parsing step\nAdding metadata such as file-name, chunk-type(image or text)\n\n\n\nContextual Summary\nThis is an additional step in RAG pipeline suggested in Anthropic’s blog, which results in better retrieval metrics. It involves generating a summary of a particular chunk w.r.t to the entire document. This summary helps locate the chunk in the document and provide necessary context about what the chunk content is about w.r.t the document. The summary is then prepended to original chunk and then embedded into the vector store\nSince, the plot-images need to converted to text itself, I didn’t generate a contextual summary for them.\n\n\nIndexing\nNow, to index the chunks, I experimented along these aspects:\n\nChoice of Embedding model: MTEB leaderboard is a good reference to compare Embedding models’ performance for different tasks. In our case, since retrieval is the main task, I tried out the following Embedding models:\n\nqwen3-embedding-8B: Its open-source and is the top-performer on the leaderboard currently. Accessed this model via Nebius’s API, due to resource constraints.\nqwen3-embedding-4B: Its open-source and provides great balance between performance & memory requirement.\ngemini-embedding-001: Accessible via API and was the top-model before Qwen3 dethroned it.\nvoyage-3-large: Suggested in Anthropic’s blog. This model can be accessed via API and they offer first 200M embedding tokens for free\nmultilingual-e5-large-instruct: Due to smaller size and it has decent performance on retrieval task\n\nChoice of Vector-store: As the name suggests, they are used to store the embedding vectors of all the chunks. I chose Qdrant due to the following reasons:\n\nOpen-source & can be run locally\nWritten in Rust which makes it fast & reliable\nFastest metadata filtering\nHigh throughput\nTo create the retriever, I used LangChain’s MultiVectorRetriever, with QdrantVectorStore as vector-store to index the chunk-embedding-vectors & InMemoryStore as doc-store to store the original document chunks\n\nChoice of Search-type: I experimented with the following search-types:\n\nSemantic search: It involves retrieving documents based on similarity scores between the query-embedding & vector-embeddings in the vector-store\nHybrid search: It combines Semantic search with Keyword-based (BM25) search, retrieving documents using RRF(Reciprocal Rank Fusion) as combination & ranking strategy.\n\n\n\n\nEvaluation\nTo log the performance of the retrieval, I created a evaluation set consisting of queries & golden-chunk-ids(ground-truth) key-value pairs against which the pipeline will be evaluated. The metric I chose in this case was Recall@k, where k={3, 5, 7}.\nFollowing are the results for the experiments conducted:\n\nEmbedding-model & Search-type choice Performance\n\n\n\n\n\n\n\nEmbedding model\nSemantic Search\nHybrid Search\n\n\n\n\nmultilingual-e5-large-instruct\nRecall@3: 81.67%  Recall@5: 90%  Recall@7: 96%\nRecall@3: 82.33%  Recall@5: 95%  Recall@7: 97%\n\n\ngemini-embedding-001\nRecall@3: 77.33%  Recall@5: 88.67%  Recall@7: 95.33%\nRecall@3: 84%  Recall@5: 93%  Recall@7: 97.67%\n\n\nqwen3-embedding-4B\nRecall@3: 79%  Recall@5: 88%  Recall@7: 94.67%\nRecall@3: 83.33%  Recall@5: 93%  Recall@7: 96.33%\n\n\nqwen3-embedding-8B\nRecall@3: 83.67%  Recall@5: 93.67%  Recall@7: 96.33%\nRecall@3: 84.67%  Recall@5: 93.33%  Recall@7: 97.33%\n\n\nvoyage-3-large\nRecall@3: 85%  Recall@5: 95.33%  Recall@7: 98.33%\nRecall@3: 86.67%  Recall@5: 93.67%  Recall@7: 97.33%\n\n\n\n\nReranking Performance\n\n\n\n\n\n\n\nEmbedding model\nWithout Re-ranking\nWith Re-ranking\n\n\n\n\nvoyage-3-large\nRecall@3: 85%  Recall@5: 95.33%  Recall@7: 98.33%\nRecall@3: 79.33%  Recall@5: 88.67%  Recall@7: 95.67%\n\n\n\n\n\nRetrieval\nBased on the results of above experiments, I made the following choices for the RAG retrieval pipeline:\n\nRetrieving only top-5 relevant chunks: This is the sweet-spot between top-3 & top-7 with a balance of observed performance & quantity of relevant information injected into the LLM context.\nVoyage-3-large as the embedding model: From the evaluation performances, Voyage-3-large provided the best metric for top-5 retrieved chunks.\nSemantic search in vector-database: Again from the evaluation performances, semantic search with Voyage-3-large embedding model was better than Hybrid search.\nNo reranking after retrieval: Re-ranking post-retrieval saw a dip in Recall@5 metric performance."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rahul Menon",
    "section": "",
    "text": "Building end-to-end pipelines where the only thing deeper than the layers is the passion."
  },
  {
    "objectID": "pages/about.html",
    "href": "pages/about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "posts/loantap.html",
    "href": "posts/loantap.html",
    "title": "LoanTap",
    "section": "",
    "text": "You can find the code for this project in this repo.\nFor this project, I followed the below Data-Science Lifecycle."
  },
  {
    "objectID": "posts/loantap.html#resources",
    "href": "posts/loantap.html#resources",
    "title": "LoanTap",
    "section": "",
    "text": "You can find the code for this project in this repo.\nFor this project, I followed the below Data-Science Lifecycle."
  },
  {
    "objectID": "posts/loantap.html#problem-statement",
    "href": "posts/loantap.html#problem-statement",
    "title": "LoanTap",
    "section": "1. Problem Statement",
    "text": "1. Problem Statement\nBanking/Lending institutions approve or reject loans of applicants based on their credit profile details. The objective of this project is to develop a classical Machine-learning model, to help the institutions approve or reject loans based on the applicant details. The impact would be reduction of manual effort in decision-making, maybe needing only manual intervention at the last stages of loan approval.\nSince, the decision is either loan approved or rejected, which is a binary feature thus making this a Binary classification task."
  },
  {
    "objectID": "posts/loantap.html#data-collection",
    "href": "posts/loantap.html#data-collection",
    "title": "LoanTap",
    "section": "2. Data collection",
    "text": "2. Data collection\nI used this Kaggle dataset for this project, hence data collection wasn’t needed.\nBefore, proceeding to the next phase, its considered a good practice to: * Choose an appropriate Success Metric * Split the data into Training & Test set and use the Test-set only in model-evaluation phase.\nUpon initial exploration of the dataset, it turns out that the data is imbalanced (Fully Paid:Charged Off :: 4:1). Hence, I chose F1-score & PR-AUC as the success metrics over Accuracy (not optimal), as they focus more on the Positive (minority) class.\nI made a stratified split to original data, which preserves the ratio of minority-majority classes in both the training & test sets."
  },
  {
    "objectID": "posts/loantap.html#eda",
    "href": "posts/loantap.html#eda",
    "title": "LoanTap",
    "section": "3. EDA",
    "text": "3. EDA\nI made use of the following tools to explore the data in the notebook:\n\nBasic Exploration\nVisualization (Univariate & Bivariate)\nOutlier detection\nCorrelation analysis\n\nHypothesis testing\n\nSome of the visualizations from the notebook are as below:   \nThe compiled insights from EDA are as follows:\n\nThe dataset has 27 features: 12 numerical, 13 categorical (including target-feature) & 2 date related in nature.\nThe dataset consists for missing values but no duplicates.\nNumercical features:\n\nNames: loan_amt, int_rate, installment, annual_inc, dti, open_acc, pub_rec, revol_bal, revol_util, total_acc, mort_acc, pub_rec_bankruptcies\nAlmost all of them are right skewed with outliers present on the high extremes. Some features such as dti, revol_bal, revol_util have outlier values which might seem impossible in real-world\nBorrowers tend to apply for loan amount in round figures i.e. 5K, 10K, 20K, 25K etc. with $10K being the most commonly borrowed amount.\nThe proportion of defaulters is higher for higher values of dti, revol_util, int_rate.\nApplicants with any negative record are more likely to default.\nThe following combination of features have shown high Spearman correlation: loan_amnt & installment, pub_rec_bankruptcies & pub_rec, total_acc & open_acc. These correlations makes sense: installment is derived from loan_amnt, pub_rec_bankruptcies & pub_rec represent similar information i.e. negative records while total_acc & open_acc represent the number of credit-lines for each applicant.\nThe median values for defaulters is slightly higher than non-defaulters in these features: int_rate, dti, loan_amnt, revol_util, open_acc. This indicates that defaulters tend to: borrow larger loans, have more open credit-lines, have higher interest rate due to high riskiness, have higher debt than income.\n\nCategorical features:\n\nNames: term, grade, sub, emp_title, emp_length, home_ownership, verification_status, purpose, title, initial_list_status, application_type, address, loan_status\nApplicants generally opt for 3-year loans than 5-year. Also, 5-year loans have higher default-rate (~50%)\nThe default rate increases with grades A-&gt;G and sub-grades A1-&gt;G5, confirming that A-grade loans are the safest while G-grade the riskiest.\nThe default rate is higher among lesser experienced applicants (&lt;1 year) while lesser among more experienced applicants.\nApplicants with rented-homes are more likely to default than the ones with mortgaged or owned homes.\nDebt-consolidation, Moving, Small-business are some of the riskiest categories of purpose feature in terms of default rate\n\nDate features:\n\nNames: issue_d, earliest_cr_line\nApplicants are with shorter credit history (2005 onwards) are more likely to default compared to applicants with longer credit history (before 1990)\n\nHypothesis Testing:\n\nFollowing numerical features are more discriminative towards loan_status & hence good predictors:\n\nFeatures with significantly higher values for Charged Off applicants: loan_amnt, int_rate, installment, dti, open_acc, pub_rec\nFeatures with significantly higher values for Fully Paid applicants: annual_inc\n\nThe outliers of following features are good indicators of default cases: loan_amnt, int_rate, annual_inc, dti, open_acc, pub_rec, revol_bal\nThe following pair of features are highly correlated with each other: loan_amnt & installment, total_acc & open_acc, pub_rec & pub_rec_bankruptcies.\nThe target feature i.e. loan_status is dependent on all the categorical features"
  },
  {
    "objectID": "posts/loantap.html#data-preprocessing",
    "href": "posts/loantap.html#data-preprocessing",
    "title": "LoanTap",
    "section": "4. Data Preprocessing",
    "text": "4. Data Preprocessing\nThis phase was executed with the objective of retaining as much information as possible from the original data. It consisted of the below steps:\n\nData cleaning\n\nThis step involved stripping whitespaces from categorical features, converting date features to date-time format, extracting pincode from address feature & merging rare categories into a single one for categorical features\n\nMissing values imputation\n\nThe imputation strategy for numerical features was median (due to outlier presence) while for most_frequent for categorical features.\nI did consider using KNNImputer, but since the dataset size is huge (~400k rows), it was time-consuming. Hence, I proceeded with the before-mentioned strategies.\n\nOutlier capping\n\nI used Winsorization (percentile-capping) to cap the outliers. Since, all the numerical features were right-skewed, I capped their upper-end with 98th percentile value.\nI also considered IQR method for outlier-capping, but for some features such as pub_rec_bankruptcies the upper-bound turned out to be 0 (most records were 0). Hence, moved ahead with the before-mentioned strategy.\n\nFeature engineering\n\nI created the following new features:\n\nemi_ratio = installment / monthly_income\ncredit_age_years = issue_d - earliest_cr_line\nclosed_acc = total_acc - open_acc\nnegative_rec = pub_rec | pub_rec_bankruptcies\ncredit_util_ratio = revol_bal / annual_inc\nmortgage_ratio = mort_acc / total_acc\n\n\nCategorical feature encoding Features for One-hot & Supervised encoding depended on the experiments.\n\nOrdinal encoding for features: grade, sub_grade, emp_length, term\nOne-hot encoding\nSupervised encoding: I ran experiments with 2 types i.e. Weight-of-evidence(WOE) & Target encoding. WOE encoding usually works well with Logistic-regression, but as per my experiment Target encoding performed better.\n\nFeature Scaling\n\nI used RobustScaler to bring the features in the same scale. This choice was made considering the presence of outliers which affects StandardScaler & MinMaxScaler. RobustScaler considers median & IQR for scaling, which are much resilient statistics towards outliers as opposed to mean & standard-deviation (used in StandardScaler)\n\nDropping irrelevant features\n\nI dropped the following features:\n\ntitle & emp_title: Due to their high-cardinality, their cleaning, information-extraction and encoding was difficult\nissue_d & earliest_cr_line: They were combined into a new feature i.e. credit_age_years and dropped later\n\n\n\nI created a scikit-learn Pipeline with the above mentioned steps using Custom-Transformers, to make experimentation easier & quicker. Find the code for the preprocessor here"
  },
  {
    "objectID": "posts/loantap.html#data-modeling",
    "href": "posts/loantap.html#data-modeling",
    "title": "LoanTap",
    "section": "5. Data Modeling",
    "text": "5. Data Modeling\nI trained the following ML models: Logistic Regression, Random-Forest, Xgboost & LightGBM. These models were trained using Cross-validation (StratifiedKFold), to make the results reliable and agnostic to the validation-split.\nThe experiments were tracked using MLFlow. Each experiment logged details such as parameter and metric values for training & validation sets. These experiments can be accessed here\nThese experiments were conducted for each model/algorithm across different aspects:\n\nHandling missing values: I tried with dropping missing values, imputing and leaving them in (specifically for tree-based models)\nEncoding strategies: I tried with only Onehot encoding, Onehot + Target encoding and only Target encoding of categorical features\nOutlier handling: I tried with capping outliers and leaving them in. Handling outliers\nClass imbalance handling: I tried with oversampling using SMOTE and class_weights parameter of models. Using class_weights parameter provided better results.\nSpecific experiments:\n\nLogistic regression assumes there is no multi-collinearity between the independent features, which I verified by calculating VIF (Variance Inflation Factor) values for each feature. I compared keeping vs. dropping features with high VIF values and the results of leaving all the features in was better, indicating that dropping those features (with VIF &gt; 5) lead to information loss affecting the predictive ability of the model.\n\n\nThese were the results of different models:\n\nTraining-set evaluation (without hyperparameter-tuning & optimization)\n\n\n\n\n\n\n\n\n\nModel\nPrecision\nRecall\nF1-score\nPR-AUC (Average Precision)\n\n\n\n\nLogistic regression\n0.522 ± 0.003\n0.791 ± 0.004\n0.629 ± 0.003\n0.783 ± 0.002\n\n\nRandom forest\n0.489 ± 0.002\n0.822 ± 0.004\n0.613 ± 0.001\n0.781 ± 0.003\n\n\nXgboost\n0.506 ± 0.002\n0.814 ± 0.003\n0.624 ± 0.002\n0.787 ± 0.002\n\n\nLightGBM\n0.503 ± 0.001\n0.815 ± 0.003\n0.622 ± 0.002\n0.786 ± 0.003\n\n\n\nConsidering Occam’s razor, Logistic regression is one of the best models including Xgboost if we had to select top-2. Next, I performed hyperparameter-tuning & optimization for these models.\nI used scikit-learn’s GridSearchCV for Logistic-regression & RandomizedSearchCV for Xgboost to find the optimal set of hyper-parameters. As part of optimization, I searched for the optimal threshold value that outputs the best metrics for Logistic-regression, Xgboost & Ensemble model (equal combination of the individual models). After threshold optimization, Ensemble model fared better than the individual models with F1-score = 67% and PR-AUC = 79%."
  },
  {
    "objectID": "posts/loantap.html#model-evaluation",
    "href": "posts/loantap.html#model-evaluation",
    "title": "LoanTap",
    "section": "6. Model Evaluation",
    "text": "6. Model Evaluation\nNow, it was time to evaluate the models on test-set held out at the beginning of the project. The results were as below:\n\nTest-set evaluation\n\n\n\n\n\n\n\n\n\nModel\nPrecision\nRecall\nF1-score\nPR-AUC (Average Precision)\n\n\n\n\nLogistic regression\n0.707\n0.613\n0.657\n0.778\n\n\nXgboost\n0.688\n0.634\n0.66\n0.78\n\n\nEnsemble (Logreg + Xgboost)\n0.664\n0.663\n0.663\n0.784\n\n\n\nBased on the above results and our success metrics i.e. F1-score & PR-AUC, I chose Ensemble model for inferencing. There is one downside though with this model that its a combination of 2 models, hence requires separate data-prerpocessing & inferencing for both which might lead to comparatively higher latency than the individual models."
  },
  {
    "objectID": "posts/loantap.html#model-deployment",
    "href": "posts/loantap.html#model-deployment",
    "title": "LoanTap",
    "section": "7. Model deployment",
    "text": "7. Model deployment\nFor deployment, I made use of Docker-compose to run 2 docker containers: Frontend (Streamlit) & Backend (FastAPI), communicating over docker-network (bridge). The compose.yaml & dockerfiles for both Frontend & Backend can be found in the repo. This can be later be easily deployed to AWS ECS Fargate.\nThe reasons for the before-mentioned deployment choices were:\n\nStreamlit: It is a Python-framework extensively used to create interactive web-applications with ease\nFastAPI: It is a excellent choice as a backend-server due to its capability to handle large number of asynchronous requests. It also automatically provides docs for the all the endpoints for faster debugging."
  }
]